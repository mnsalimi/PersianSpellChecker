{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96775e5b",
   "metadata": {},
   "source": [
    "<center dir=\"rtl\" style='font-size: 20px'>به نام خدا</center>\n",
    "<br>\n",
    "<center dir=\"rtl\" style='font-size: 16px'>تمرین سوم: بخش استخراج دیتا</center>\n",
    "<br>\n",
    "<center dir=\"rtl\" style='font-size: 14px'>معین سلیمی - سحرزال - حسین پرتو</center>\n",
    "<div style='font-size: 16px' dir=rtl>در این نوتبوک به دو کار پرداخته شده‌است:</div>\n",
    "<ul dir=\"rtl\">\n",
    "    <li>\n",
    "        استخراج مجموعه کلمات از دیتاست آرمان و ذخیره آن به صورت پیکل جهت استفاده در مدل پایه بر مبنای فاصله ویرایشی\n",
    "    </li>\n",
    "    <li>\n",
    "        تولید حدود 10 هزار تست‌کیس به صورت تصادفی از بین دیتای اخبار سیاسی موجود در تمرین\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "808bbc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pandas\n",
    "!pip install -q hazm\n",
    "!pip install -q dadmatools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0225309d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import codecs\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be19d8f6",
   "metadata": {},
   "source": [
    "<h2 dir=\"rtl\">1- استخراج مجموعه کلمات</h2>\n",
    "<br>\n",
    "<div dir=\"rtl\">در این قسمت کلمات را از درون فایل‌های دیتاست آرمان می‌خوانیم و هریک را در لیست ذخیره می‌کنیم:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ad9d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = [x.strip().split() for x in tqdm.tqdm(codecs.open('/SUT/term1/NLP/module2Code/data/train_fold1.txt','rU','utf-8').readlines())]\n",
    "data2 = [x.strip().split() for x in tqdm.tqdm(codecs.open('/SUT/term1/NLP/module2Code/data/train_fold2.txt','rU','utf-8').readlines())]\n",
    "data3 = [x.strip().split() for x in tqdm.tqdm(codecs.open('/SUT/term1/NLP/module2Code/data/train_fold3.txt','rU','utf-8').readlines())]\n",
    "data4 = [x.strip().split() for x in tqdm.tqdm(codecs.open('/SUT/term1/NLP/module2Code/data/test_fold1.txt','rU','utf-8').readlines())]\n",
    "data5 = [x.strip().split() for x in tqdm.tqdm(codecs.open('/SUT/term1/NLP/module2Code/data/test_fold2.txt','rU','utf-8').readlines())]\n",
    "data6 = [x.strip().split() for x in tqdm.tqdm(codecs.open('/SUT/term1/NLP/module2Code/data/test_fold3.txt','rU','utf-8').readlines())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62fbcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for word in data1:\n",
    "    if(len(word) > 0):\n",
    "        words.append(word[0])\n",
    "        \n",
    "for word in data2:\n",
    "    if(len(word) > 0):\n",
    "        words.append(word[0])\n",
    "        \n",
    "for word in data3:\n",
    "    if(len(word) > 0):\n",
    "        words.append(word[0])\n",
    "        \n",
    "for word in data4:\n",
    "    if(len(word) > 0):\n",
    "        words.append(word[0])\n",
    "        \n",
    "        \n",
    "for word in data5:\n",
    "    if(len(word) > 0):\n",
    "        words.append(word[0])\n",
    "        \n",
    "for word in data6:\n",
    "    if(len(word) > 0):\n",
    "        words.append(word[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0dacd6",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">در این قسمت دیتا را به صورت دیکشنری در می‌آوریم تا از کلمات تکراری جلوگیری کنیم:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b342edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_dict = dict.fromkeys(words,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a2e03e",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">در این قسمت دیتا را به صورت پیکل ذخیره می‌کنیم:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228b2c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "words_list = list(words_dict.keys())\n",
    "with open(\"words.pickle\", \"wb\") as f:\n",
    "        pickle.dump(words_list, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437eff2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "politics = [x.strip().split() for x in tqdm.tqdm(codecs.open('/SUT/term1/NLP/module2Code/data/politics.txt','rU','utf-8').readlines())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c24054",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(politics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fef9ee7",
   "metadata": {},
   "source": [
    "<h2 dir=\"rtl\">2- تولید تست‌کیس‌های تصادفی</h2>\n",
    "<br>\n",
    "<div dir=\"rtl\">در این قسمت همه اخبار موجود درون اخبار سیاسی دیتاستی که در تمرین قرار گرفته شده‌است را از فایل آن می‌خوانیم و آن‌ها را به صورت جملاتی که با نقطه تفکیک شده‌اند ذخیره میکنیم:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87580a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 543666/543666 [00:01<00:00, 407896.71it/s]\n"
     ]
    }
   ],
   "source": [
    "politics_sentence = [x.split(\".\") for x in tqdm.tqdm(codecs.open('/SUT/term1/NLP/module2Code/data/politics.txt','rU','utf-8').readlines())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5611886",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1552310"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_news_sentences = []\n",
    "all_news_sentences_origin = []\n",
    "for news in politics_sentence:\n",
    "    for sent in news:\n",
    "        if sent != '\\n' and  sent != '':\n",
    "            all_news_sentences.append(sent)\n",
    "            all_news_sentences_origin.append(sent)\n",
    "len(all_news_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fc472a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7cea05cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ایسنا:',\n",
       " 'رئیس\\u200cجمهور',\n",
       " 'در',\n",
       " 'جلسه',\n",
       " 'ستاد',\n",
       " 'ملی',\n",
       " 'مقابله',\n",
       " 'با',\n",
       " 'کرونا',\n",
       " 'با',\n",
       " 'بیان',\n",
       " 'اینکه',\n",
       " 'تمهیدات',\n",
       " 'آغاز',\n",
       " 'آموزش',\n",
       " 'حضوری',\n",
       " 'در',\n",
       " 'مدارس',\n",
       " 'صورت',\n",
       " 'گرفته',\n",
       " 'است،',\n",
       " 'گفت:',\n",
       " 'در',\n",
       " 'کنار',\n",
       " 'اقدامات',\n",
       " 'انجام\\u200cشده',\n",
       " 'برای',\n",
       " 'واکسیناسیون',\n",
       " 'معلمان،',\n",
       " 'کادر',\n",
       " 'مدارس،',\n",
       " 'والدین',\n",
       " 'و',\n",
       " 'رانندگان',\n",
       " 'سرویس',\n",
       " 'مدارس،',\n",
       " 'لازم',\n",
       " 'است',\n",
       " 'واکسیناسیون',\n",
       " 'دانش\\u200cآموزان',\n",
       " 'بالای',\n",
       " '12',\n",
       " 'سال',\n",
       " 'نیز',\n",
       " 'با',\n",
       " 'سرعت',\n",
       " 'انجام',\n",
       " 'شود']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_news_sentences_words = [x.strip().split() for x in all_news_sentences[0:10000]]\n",
    "all_news_sentences_words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "271de7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_news = all_news_sentences_words\n",
    "some_news_original = all_news_sentences_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "88d90ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for replacements in next section\n",
    "chars = [\"ا\",\"ب\",\"پ\",\"ت\",\"ث\",\"ج\",\"چ\",\"ح\",\"خ\",\"د\",\"ر\",\"ژ\",\"ز\",\"ش\",\"س\",\"ص\",\"ض\",\"ط\",\"ظ\",\"ع\",\"غ\",\"ق\",\"ف\",\"ک\",\"گ\",\"ل\",\"م\",\"ن\",\"و\",\"ه\",]\n",
    "#we dont want these chars to be replaced or misplaced\n",
    "stops = [\".\",\"/\",\";\",\"(\",\")\",\":\",\" \",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"0\",\"»\",\"«\",\"۰\",\"۱\",\"۲\",\"۳\",\"۴\",\"۵\",\"۶\",\"۷\",\"۸\",\"۹\"]\n",
    "#we dont want these words to be replaced or misplaced ( too simple )\n",
    "stop_words = [\"نیز\",\"ها\",\"در\",\"و\",\"از\",\"به\",\"تا\",\"با\",\"باید\",\"\\n\",\"اما\",\"اگر\",\"را\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d97f3f",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\" style='font-size: 16px'>\n",
    "در این قسمت میخواهیم به صورت رندوم در برخی از کلمات هر جمله خطای تصادفی ایجاد کنیم که برای آن یکسری قوانین در نظر گرفته شده‌است به عنوان مثال در جملات کمتر از ۵ کلمه خطا در ۰ تا ۱ کلمه رخ می‌دهد و در جملات کمتر از ۱۰ کلمه در ۰ تا ۲ کلمه خطا رخ می‌دهد.\n",
    "    اینکه در کدام یک از کاراکترها خطایی رخ دهد نیز به صورت تصادفی انتخاب می‌شود. و در هر یک از کلمات فقط یکی از خطاهای زیر رخ می‌دهد.\n",
    "</div>\n",
    "\n",
    "<div style='font-size: 16px' dir=rtl>خطاهای صورت گرفته به صورت زیر است:</div>\n",
    "<ul dir=\"rtl\">\n",
    "    <li>\n",
    "        جابه‌جایی کاراکتر با یک کاراکتر رندوم که از لیست کاراکترهای فارسی در قسمت قبلی انتخاب می‌شود\n",
    "    </li>\n",
    "    <li>\n",
    "    حذف یکی از کاراکترها\n",
    "    </li>\n",
    "        <li>\n",
    "    جابه‌جایی دو کاراکتر\n",
    "    </li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed395244",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                        | 0/10000 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "import random\n",
    "# counter = 0\n",
    "for sent in tqdm.tqdm(some_news[0:10000]):\n",
    "    how_many_words = random.randrange(0, 4,1)\n",
    "    if len(sent) < 5:\n",
    "        how_many_words = random.randrange(0, 2)\n",
    "    elif len(sent) < 10:\n",
    "        how_many_words = random.randrange(0, 3)\n",
    "    i = 0\n",
    "    selected_words = []\n",
    "    while i < how_many_words and len(sent) > 0:\n",
    "        word_index = random.randrange(0, len(sent),1)\n",
    "        if sent[word_index] not in stop_words and len(sent[word_index]) > 2 :\n",
    "            char_index = random.randrange(0,len(sent[word_index]),1)\n",
    "            if (sent[word_index][char_index] not in stops) and (sent[word_index] not in selected_words):\n",
    "                selected_words.append(sent[word_index])\n",
    "                random_work = random.randrange(1,5,1)\n",
    "#                 counter += 1\n",
    "                if random_work == 1 or random_work == 3: # replace by random\n",
    "                    random_char = random.randrange(0,30,1)\n",
    "                    sent[word_index] =sent[word_index][:char_index]+ chars[random_char] +sent[word_index][char_index+1:]\n",
    "                elif random_work == 2: # delete\n",
    "                    sent[word_index] = sent[word_index][:char_index] + sent[word_index][char_index+1:]\n",
    "                else: # replace by prev or next\n",
    "                    if char_index == (len(sent[word_index]) - 1):\n",
    "                        char_index -= 1\n",
    "                    elif char_index == 0:\n",
    "                        char_index = 1\n",
    "                    prev_char = sent[word_index][char_index - 1]\n",
    "                    self_char = sent[word_index][char_index]\n",
    "                    sent[word_index] = sent[word_index][:char_index - 1] + self_char + prev_char + sent[word_index][char_index+1:]\n",
    "                i+=1\n",
    "# some_news      \n",
    "# counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fe1b18",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">دیتای خطادار و دیتای صحیح را در یک فایل سی‌اس‌وی ذخیره میکنیم تا بتوانیم در تست‌ها از آن‌ها استفاده کنیم:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cf231b",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_news_wrong_all = [' '.join(x) for x in some_news]\n",
    "some_news_wrong_all[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f99de3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "alls = [x.strip() for x in all_news_sentences[0:10000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6e534b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_data = []\n",
    "for i in range(len(alls)):\n",
    "    all_test_data.append([alls[i],some_news_wrong_all[i]])\n",
    "    \n",
    "all_test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6942c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open(\"test_data.csv\",\"w+\",encoding=\"utf-8\") as my_csv:\n",
    "    csvWriter = csv.writer(my_csv,delimiter=',')\n",
    "    csvWriter.writerows(all_test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
